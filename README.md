The NYC Taxi Data Engineering Project is a complete end-to-end data engineering implementation that demonstrates how to design, build, and automate modern data pipelines using Azure Data Factory (ADF), Azure Databricks, Azure Data Lake Storage Gen2 (ADLS), and Delta Lake. The project focuses on ingesting, transforming, and serving real-world data efficiently by applying industry-standard architectures and best practices. It is designed to make learners industry-ready by covering in-demand tools, real-time scenarios, and complex automation patterns used by professional data engineers.

The project begins with a clear objective — to automate the ingestion of monthly NYC Green Taxi Trip Data directly from the public API provided by the NYC Open Data portal. Instead of manually downloading files, the solution dynamically pulls parquet-formatted datasets using HTTP linked services within Azure Data Factory. This approach reflects a real-world enterprise scenario where data ingestion must be scheduled and automated, ensuring scalability and reliability. The ingested data is stored in Azure Data Lake Storage Gen2 (ADLS), which serves as the centralized data repository. ADLS is chosen for its support of hierarchical namespaces, which enables folder-like directory structures and makes it suitable for big data processing and multi-layered storage design.

The architecture follows the Medallion Architecture pattern, which organizes data into three logical layers — Bronze, Silver, and Gold. The Bronze layer stores raw ingested data exactly as received from the source. The Silver layer contains cleaned and enriched data after applying data validation and standardization steps. Finally, the Gold layer holds the curated, business-ready data used for analytics, reporting, and machine learning applications. This layered approach allows clear separation of data transformation stages, simplifies data governance, and ensures reliability and reusability of processed data.

In the first phase, a Resource Group is created in Azure to hold all project resources, including ADF, ADLS, and Databricks. A new storage account is then provisioned with the hierarchical namespace option enabled, effectively converting it into a Data Lake Gen2. Three containers — bronze, silver, and gold — are created to represent the medallion layers. Additional folders within these containers are created for specific datasets such as trip_type and trip_zone, which include CSV lookup tables required for data enrichment. These files are uploaded manually as initial static resources to support later transformation steps.

In the data ingestion phase, an Azure Data Factory instance is created to orchestrate the entire data movement. Within ADF, linked services are configured to establish secure connections to the NYC open data source via HTTP, and to the Data Lake Gen2 storage as the sink. Each linked service acts as a connection profile that ADF uses to authenticate and interact with external systems. Once connections are established, datasets are defined. The source dataset uses the HTTP linked service and is parameterized to accept a relative URL representing the specific monthly parquet file (e.g., green_tripdata_2023-01.parquet). The sink dataset references the ADLS Gen2 linked service, specifying the path to the bronze container where ingested files will be stored.

A Copy Activity is created to transfer data from the source (HTTP) to the sink (ADLS). However, instead of building twelve separate pipelines for each month, a dynamic and parameterized pipeline is developed. This is achieved by introducing a ForEach activity within ADF that iterates through a range of months (from 1 to 12) using the built-in range() function. The ForEach loop passes the current iteration value (representing the month) as a parameter to the HTTP dataset, dynamically constructing the full file URL for each monthly file. The pipeline thus automatically downloads all 12 monthly parquet files for the year 2023 in a single execution. This parameterized design exemplifies a key principle of modern data engineering — automation and scalability through dynamic orchestration.

After successful ingestion, the parquet files are stored in the bronze container of the Data Lake. The next phase of the project focuses on data transformation and enrichment using Azure Databricks and PySpark. Databricks is chosen for its ability to handle large-scale distributed data processing and its seamless integration with Azure ecosystem components. A Databricks workspace and cluster are created, and a series of notebooks are developed to process the ingested data.

In the transformation phase, Databricks reads the parquet files from the bronze container using the spark.read.parquet() function. The data undergoes cleansing, type casting, and filtering operations. Missing values are handled, timestamp columns are properly formatted, and invalid trip records are removed. Lookup data from the zone CSV file is joined with the trip dataset to enrich the data with geographic zone details. The cleaned and enriched data is then written into the silver container using the .write.parquet() method, resulting in a structured and quality-checked dataset ready for analytical processing.

In the final transformation phase, data is further refined into the gold layer. Here, aggregation and summarization operations are performed to produce business-level datasets such as total trips per zone, average fare per distance, and trip distribution by time of day. The gold data is stored in Delta Lake format, an open-source storage layer built on top of Parquet. Delta Lake provides ACID transactions, schema enforcement, and time travel capabilities, ensuring that data integrity is maintained even during concurrent write operations.

Delta Lake’s transaction log (Delta Log) automatically tracks all changes to the data, supporting data versioning and rollback features. 
gitThis feature is especially useful for debugging, auditing, and historical data analysis. These Delta capabilities significantly enhance data reliability, reproducibility, and auditability — all critical components of enterprise-grade data engineering solutions.

Once transformations are complete, the final gold Delta tables are registered within Databricks as external tables, making them queryable using Spark SQL or accessible from external visualization tools like Power BI. Instead of building dashboards within Databricks, the project focuses on exposing clean, ready-to-use data for analytics teams through secure data connections. Power BI connects to Databricks using direct queries, enabling real-time visualization of trip patterns and insights derived from the gold layer.

Throughout the implementation, attention is paid to security and governance. Managed Identities and role-based access controls are applied to restrict unauthorized access to storage resources. Redundancy options such as Locally Redundant Storage (LRS) and Geo-Redundant Storage (GRS) are discussed and configured according to cost and reliability requirements. These redundancy strategies ensure high data availability and protection against data center failures.

Pipeline monitoring is achieved using ADF’s Monitor tab, which tracks the status, duration, and performance of each pipeline execution. Databricks provides its own job monitoring and cluster logs to ensure smooth transformations. Each step of the process — from ingestion to gold-level Delta creation — can be monitored, debugged, and optimized using these tools.

By the end of the project, the architecture achieves a fully automated, dynamic, and production-grade data pipeline that demonstrates key industry patterns: data ingestion automation, parameterized orchestration, distributed data transformation, Delta Lake version control, and data serving for downstream analytics. The project reflects how professional data engineers design scalable and reliable data ecosystems on the Azure platform.

From a technical learning perspective, this project provides hands-on exposure to critical concepts such as Linked Services, Datasets, Pipelines, ForEach loops, Parameterization, Hierarchical Namespaces, Medallion Architecture, and Delta Lake operations. It emphasizes reusability, modularity, and scalability — ensuring that the same architecture can be adapted for other domains like finance, healthcare, or IoT data ingestion.

In conclusion, the NYC Taxi Data Engineering Project is a complete real-world demonstration of modern data engineering principles. It combines Azure Data Factory for orchestration, Azure Data Lake for storage, Azure Databricks for transformation, and Delta Lake for reliable data management. The pipeline efficiently automates ingestion of large-scale datasets, applies complex transformations, and outputs clean, versioned, and query-ready data that supports analytics and business intelligence. This project not only strengthens one’s practical cloud data engineering skills but also provides an impressive portfolio piece showcasing the ability to design end-to-end data solutions aligned with enterprise data engineering standards.